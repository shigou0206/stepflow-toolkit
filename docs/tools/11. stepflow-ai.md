# Stepflow AI Package

## 概述

`stepflow-ai` 是 Stepflow Tool System 的 AI 工具包，负责集成和管理各种 AI 模型和服务的工具。该包支持多种 AI 提供商（OpenAI、Anthropic、Google、本地模型等），提供模型管理、提示工程、推理执行和结果处理功能。

## 包结构

```
stepflow-ai/
├── src/
│   ├── lib.rs                 # 包入口和公共接口
│   ├── provider.rs            # AI 提供商管理
│   ├── model.rs               # 模型管理
│   ├── executor.rs            # AI 执行器
│   ├── prompt.rs              # 提示工程
│   ├── inference.rs           # 推理执行
│   ├── streaming.rs           # 流式处理
│   ├── embedding.rs           # 嵌入处理
│   ├── security.rs            # 安全控制
│   └── errors.rs              # 错误类型定义
├── tests/
│   ├── integration_tests.rs   # 集成测试
│   ├── unit_tests.rs         # 单元测试
│   └── fixtures/             # 测试数据
└── Cargo.toml
```

## 核心接口定义

### AIProvider 特征

```rust
#[async_trait]
pub trait AIProvider: Send + Sync {
    /// 获取提供商信息
    async fn get_provider_info(&self) -> Result<ProviderInfo, AIError>;
    
    /// 列出可用模型
    async fn list_models(&self) -> Result<Vec<ModelInfo>, AIError>;
    
    /// 获取模型信息
    async fn get_model_info(&self, model_id: &str) -> Result<ModelInfo, AIError>;
    
    /// 检查模型可用性
    async fn check_model_availability(&self, model_id: &str) -> Result<bool, AIError>;
}
```

### AIExecutor 特征

```rust
#[async_trait]
pub trait AIExecutor: Send + Sync {
    /// 执行文本生成
    async fn generate_text(&self, request: TextGenerationRequest) -> Result<TextGenerationResponse, AIError>;
    
    /// 执行聊天对话
    async fn chat_completion(&self, request: ChatRequest) -> Result<ChatResponse, AIError>;
    
    /// 执行流式生成
    async fn stream_generation(&self, request: TextGenerationRequest) -> Result<TextStream, AIError>;
    
    /// 执行嵌入生成
    async fn generate_embedding(&self, request: EmbeddingRequest) -> Result<EmbeddingResponse, AIError>;
    
    /// 执行图像生成
    async fn generate_image(&self, request: ImageGenerationRequest) -> Result<ImageGenerationResponse, AIError>;
}
```

### AIModelManager 特征

```rust
#[async_trait]
pub trait AIModelManager: Send + Sync {
    /// 注册模型
    async fn register_model(&self, model: ModelInfo) -> Result<ModelId, AIError>;
    
    /// 获取模型
    async fn get_model(&self, model_id: &ModelId) -> Result<ModelInfo, AIError>;
    
    /// 更新模型
    async fn update_model(&self, model_id: &ModelId, updates: ModelUpdates) -> Result<(), AIError>;
    
    /// 删除模型
    async fn delete_model(&self, model_id: &ModelId) -> Result<(), AIError>;
    
    /// 列出所有模型
    async fn list_models(&self, filter: Option<ModelFilter>) -> Result<Vec<ModelInfo>, AIError>;
}
```

## 数据结构

### ModelInfo

```rust
#[derive(Debug, Clone)]
pub struct ModelInfo {
    pub id: ModelId,
    pub name: String,
    pub provider: String,
    pub model_type: ModelType,
    pub capabilities: Vec<ModelCapability>,
    pub parameters: ModelParameters,
    pub pricing: Option<PricingInfo>,
    pub status: ModelStatus,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}
```

### TextGenerationRequest

```rust
#[derive(Debug, Clone)]
pub struct TextGenerationRequest {
    pub model_id: ModelId,
    pub prompt: String,
    pub parameters: GenerationParameters,
    pub safety_settings: Option<SafetySettings>,
    pub streaming: bool,
    pub metadata: HashMap<String, Value>,
}
```

### ChatRequest

```rust
#[derive(Debug, Clone)]
pub struct ChatRequest {
    pub model_id: ModelId,
    pub messages: Vec<ChatMessage>,
    pub parameters: GenerationParameters,
    pub system_prompt: Option<String>,
    pub safety_settings: Option<SafetySettings>,
    pub streaming: bool,
    pub metadata: HashMap<String, Value>,
}
```

### ChatMessage

```rust
#[derive(Debug, Clone)]
pub struct ChatMessage {
    pub role: ChatRole,
    pub content: String,
    pub name: Option<String>,
    pub function_call: Option<FunctionCall>,
    pub tool_calls: Option<Vec<ToolCall>>,
}

#[derive(Debug, Clone)]
pub enum ChatRole {
    System,
    User,
    Assistant,
    Function,
    Tool,
}
```

### GenerationParameters

```rust
#[derive(Debug, Clone)]
pub struct GenerationParameters {
    pub temperature: f32,
    pub max_tokens: Option<u32>,
    pub top_p: f32,
    pub top_k: Option<u32>,
    pub frequency_penalty: f32,
    pub presence_penalty: f32,
    pub stop_sequences: Vec<String>,
    pub seed: Option<u64>,
}
```

## 数据库模式

### ai_models 表

```sql
CREATE TABLE ai_models (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    provider TEXT NOT NULL,
    model_type TEXT NOT NULL,
    capabilities TEXT, -- JSON
    parameters TEXT, -- JSON
    pricing TEXT, -- JSON
    status TEXT NOT NULL DEFAULT 'active',
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);
```

### ai_executions 表

```sql
CREATE TABLE ai_executions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    execution_id TEXT NOT NULL,
    model_id TEXT NOT NULL,
    request_type TEXT NOT NULL, -- 'text_generation', 'chat', 'embedding', 'image'
    request_data TEXT NOT NULL, -- JSON
    response_data TEXT, -- JSON
    tokens_used INTEGER,
    cost REAL,
    duration_ms INTEGER,
    status TEXT NOT NULL,
    error_message TEXT,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (execution_id) REFERENCES executions(id) ON DELETE CASCADE,
    FOREIGN KEY (model_id) REFERENCES ai_models(id) ON DELETE CASCADE
);
```

### ai_providers 表

```sql
CREATE TABLE ai_providers (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    api_base_url TEXT,
    api_key TEXT,
    config TEXT, -- JSON
    status TEXT NOT NULL DEFAULT 'active',
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);
```

### ai_prompts 表

```sql
CREATE TABLE ai_prompts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tool_id TEXT NOT NULL,
    prompt_name TEXT NOT NULL,
    prompt_template TEXT NOT NULL,
    variables TEXT, -- JSON
    examples TEXT, -- JSON
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (tool_id) REFERENCES tools(id) ON DELETE CASCADE
);
```

## 核心实现

### AIProviderImpl

```rust
pub struct AIProviderImpl {
    db: Arc<Database>,
    config: ProviderConfig,
}

impl AIProviderImpl {
    pub fn new(db: Arc<Database>, config: ProviderConfig) -> Self {
        Self {
            db,
            config,
        }
    }
    
    /// 获取提供商信息
    async fn get_provider_info_internal(&self) -> Result<ProviderInfo, AIError> {
        let provider_id = self.config.provider_id.clone();
        let provider = self.get_provider_from_db(&provider_id).await?;
        
        Ok(ProviderInfo {
            id: provider_id,
            name: provider.name,
            api_base_url: provider.api_base_url,
            capabilities: self.parse_capabilities(&provider.config).await?,
            status: provider.status,
        })
    }
    
    /// 列出可用模型
    async fn list_models_internal(&self) -> Result<Vec<ModelInfo>, AIError> {
        let models = self.get_models_from_db().await?;
        
        // 过滤活跃模型
        let active_models: Vec<ModelInfo> = models
            .into_iter()
            .filter(|model| model.status == ModelStatus::Active)
            .collect();
        
        Ok(active_models)
    }
}
```

### AIExecutorImpl

```rust
pub struct AIExecutorImpl {
    db: Arc<Database>,
    provider_manager: Arc<dyn AIProvider>,
    model_manager: Arc<dyn AIModelManager>,
    config: ExecutorConfig,
}

impl AIExecutorImpl {
    pub fn new(
        db: Arc<Database>,
        provider_manager: Arc<dyn AIProvider>,
        model_manager: Arc<dyn AIModelManager>,
        config: ExecutorConfig,
    ) -> Self {
        Self {
            db,
            provider_manager,
            model_manager,
            config,
        }
    }
    
    /// 执行文本生成
    async fn generate_text_internal(&self, request: TextGenerationRequest) -> Result<TextGenerationResponse, AIError> {
        // 验证模型
        let model = self.model_manager.get_model(&request.model_id).await?;
        
        // 验证请求参数
        self.validate_generation_request(&request, &model).await?;
        
        // 获取提供商客户端
        let client = self.get_provider_client(&model.provider).await?;
        
        // 执行生成
        let start_time = Instant::now();
        let response = client.generate_text(&request).await?;
        let duration = start_time.elapsed();
        
        // 记录执行
        self.record_execution(&request, &response, duration).await?;
        
        Ok(response)
    }
    
    /// 执行聊天对话
    async fn chat_completion_internal(&self, request: ChatRequest) -> Result<ChatResponse, AIError> {
        // 验证模型
        let model = self.model_manager.get_model(&request.model_id).await?;
        
        // 验证聊天请求
        self.validate_chat_request(&request, &model).await?;
        
        // 获取提供商客户端
        let client = self.get_provider_client(&model.provider).await?;
        
        // 执行聊天
        let start_time = Instant::now();
        let response = client.chat_completion(&request).await?;
        let duration = start_time.elapsed();
        
        // 记录执行
        self.record_chat_execution(&request, &response, duration).await?;
        
        Ok(response)
    }
}
```

### AIModelManagerImpl

```rust
pub struct AIModelManagerImpl {
    db: Arc<Database>,
    config: ModelManagerConfig,
}

impl AIModelManagerImpl {
    pub fn new(db: Arc<Database>, config: ModelManagerConfig) -> Self {
        Self {
            db,
            config,
        }
    }
    
    /// 注册模型
    async fn register_model_internal(&self, model: ModelInfo) -> Result<ModelId, AIError> {
        // 验证模型信息
        self.validate_model_info(&model).await?;
        
        // 检查模型是否已存在
        if self.model_exists(&model.name).await? {
            return Err(AIError::ModelAlreadyExists(model.name));
        }
        
        // 保存模型到数据库
        let model_id = self.save_model_to_db(&model).await?;
        
        // 更新模型状态
        self.update_model_status(&model_id, ModelStatus::Active).await?;
        
        Ok(model_id)
    }
    
    /// 获取模型
    async fn get_model_internal(&self, model_id: &ModelId) -> Result<ModelInfo, AIError> {
        let model = self.get_model_from_db(model_id).await?;
        
        if model.status == ModelStatus::Inactive {
            return Err(AIError::ModelNotAvailable(model_id.clone()));
        }
        
        Ok(model)
    }
}
```

## 错误类型

```rust
#[derive(Debug, thiserror::Error)]
pub enum AIError {
    #[error("Model not found: {0}")]
    ModelNotFound(ModelId),
    
    #[error("Model not available: {0}")]
    ModelNotAvailable(ModelId),
    
    #[error("Model already exists: {0}")]
    ModelAlreadyExists(String),
    
    #[error("Provider not found: {0}")]
    ProviderNotFound(String),
    
    #[error("Provider not available: {0}")]
    ProviderNotAvailable(String),
    
    #[error("Invalid request: {0}")]
    InvalidRequest(String),
    
    #[error("Generation failed: {0}")]
    GenerationFailed(String),
    
    #[error("Rate limit exceeded")]
    RateLimitExceeded,
    
    #[error("Token limit exceeded")]
    TokenLimitExceeded,
    
    #[error("Safety violation: {0}")]
    SafetyViolation(String),
    
    #[error("Authentication failed: {0}")]
    AuthenticationFailed(String),
    
    #[error("Network error: {0}")]
    NetworkError(String),
    
    #[error("Database error: {0}")]
    DatabaseError(#[from] DatabaseError),
    
    #[error("Internal error: {0}")]
    InternalError(String),
}
```

## 配置管理

### ProviderConfig

```rust
#[derive(Debug, Clone)]
pub struct ProviderConfig {
    pub provider_id: String,
    pub api_key: Option<String>,
    pub api_base_url: Option<String>,
    pub timeout: Duration,
    pub max_retries: u32,
    pub retry_delay: Duration,
    pub enable_logging: bool,
}
```

### ExecutorConfig

```rust
#[derive(Debug, Clone)]
pub struct ExecutorConfig {
    pub default_model: Option<ModelId>,
    pub max_tokens_per_request: u32,
    pub max_requests_per_minute: u32,
    pub enable_streaming: bool,
    pub enable_safety_checks: bool,
    pub enable_cost_tracking: bool,
    pub default_parameters: GenerationParameters,
}
```

### ModelManagerConfig

```rust
#[derive(Debug, Clone)]
pub struct ModelManagerConfig {
    pub auto_refresh_models: bool,
    pub refresh_interval: Duration,
    pub enable_model_caching: bool,
    pub cache_ttl: Duration,
    pub max_models_per_provider: usize,
}
```

## AI 提供商支持

### OpenAI 支持

```rust
pub struct OpenAIProvider {
    client: OpenAIClient,
    config: OpenAIConfig,
}

impl OpenAIProvider {
    pub fn new(config: OpenAIConfig) -> Result<Self, AIError> {
        let client = OpenAIClient::new(&config.api_key)?;
        Ok(Self { client, config })
    }
    
    async fn generate_text(&self, request: &TextGenerationRequest) -> Result<TextGenerationResponse, AIError> {
        let response = self.client.completions()
            .create(request.to_openai_request())
            .await?;
        
        Ok(TextGenerationResponse::from_openai_response(response))
    }
}
```

### Anthropic 支持

```rust
pub struct AnthropicProvider {
    client: AnthropicClient,
    config: AnthropicConfig,
}

impl AnthropicProvider {
    pub fn new(config: AnthropicConfig) -> Result<Self, AIError> {
        let client = AnthropicClient::new(&config.api_key)?;
        Ok(Self { client, config })
    }
    
    async fn chat_completion(&self, request: &ChatRequest) -> Result<ChatResponse, AIError> {
        let response = self.client.messages()
            .create(request.to_anthropic_request())
            .await?;
        
        Ok(ChatResponse::from_anthropic_response(response))
    }
}
```

## 流式处理

### TextStream

```rust
pub struct TextStream {
    pub stream_id: StreamId,
    pub model_id: ModelId,
    pub stream: Pin<Box<dyn Stream<Item = Result<TextChunk, AIError>> + Send>>,
}

impl TextStream {
    pub fn new(stream_id: StreamId, model_id: ModelId, stream: impl Stream<Item = Result<TextChunk, AIError>> + Send + 'static) -> Self {
        Self {
            stream_id,
            model_id,
            stream: Box::pin(stream),
        }
    }
}
```

### Streaming 特征

```rust
#[async_trait]
pub trait StreamingExecutor: Send + Sync {
    async fn stream_text_generation(&self, request: TextGenerationRequest) -> Result<TextStream, AIError>;
    async fn stream_chat_completion(&self, request: ChatRequest) -> Result<ChatStream, AIError>;
    async fn cancel_stream(&self, stream_id: &StreamId) -> Result<(), AIError>;
}
```

## 安全控制

### SafetySettings

```rust
#[derive(Debug, Clone)]
pub struct SafetySettings {
    pub harassment: SafetyLevel,
    pub hate_speech: SafetyLevel,
    pub sexually_explicit: SafetyLevel,
    pub dangerous_content: SafetyLevel,
    pub custom_filters: Vec<CustomFilter>,
}

#[derive(Debug, Clone)]
pub enum SafetyLevel {
    BlockNone,
    BlockLow,
    BlockMedium,
    BlockHigh,
}
```

### SafetyValidator 特征

```rust
#[async_trait]
pub trait SafetyValidator: Send + Sync {
    async fn validate_prompt(&self, prompt: &str) -> Result<SafetyResult, AIError>;
    async fn validate_response(&self, response: &str) -> Result<SafetyResult, AIError>;
    async fn check_content_policy(&self, content: &str) -> Result<PolicyResult, AIError>;
}
```

## 监控和指标

### AIMetrics

```rust
#[derive(Debug, Clone)]
pub struct AIMetrics {
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub average_response_time: Duration,
    pub total_tokens_used: u64,
    pub total_cost: f64,
    pub active_models: u64,
    pub rate_limit_hits: u64,
}
```

### Monitoring 特征

```rust
#[async_trait]
pub trait AIMonitoring: Send + Sync {
    async fn record_request(&self, model_id: &ModelId, request_type: &str, duration: Duration, success: bool) -> Result<(), MonitoringError>;
    async fn record_token_usage(&self, model_id: &ModelId, tokens_used: u32, cost: f64) -> Result<(), MonitoringError>;
    async fn record_safety_violation(&self, model_id: &ModelId, violation_type: &str) -> Result<(), MonitoringError>;
    async fn get_metrics(&self, filter: Option<MetricFilter>) -> Result<Vec<Metric>, MonitoringError>;
}
```

## 测试标准

### 单元测试

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_text_generation() {
        // 测试文本生成
    }
    
    #[tokio::test]
    async fn test_chat_completion() {
        // 测试聊天对话
    }
    
    #[tokio::test]
    async fn test_model_registration() {
        // 测试模型注册
    }
    
    #[tokio::test]
    async fn test_safety_validation() {
        // 测试安全验证
    }
}
```

### 集成测试

```rust
#[cfg(test)]
mod integration_tests {
    use super::*;

    #[tokio::test]
    async fn test_full_ai_workflow() {
        // 测试完整 AI 工作流
    }
    
    #[tokio::test]
    async fn test_streaming_generation() {
        // 测试流式生成
    }
    
    #[tokio::test]
    async fn test_multiple_providers() {
        // 测试多提供商
    }
}
```

## 部署和配置

### 环境变量

```bash
# AI 提供商配置
STEPFLOW_AI_OPENAI_API_KEY=your-openai-key
STEPFLOW_AI_ANTHROPIC_API_KEY=your-anthropic-key
STEPFLOW_AI_GOOGLE_API_KEY=your-google-key
STEPFLOW_AI_DEFAULT_PROVIDER=openai

# AI 执行器配置
STEPFLOW_AI_MAX_TOKENS_PER_REQUEST=4000
STEPFLOW_AI_MAX_REQUESTS_PER_MINUTE=60
STEPFLOW_AI_ENABLE_STREAMING=true
STEPFLOW_AI_ENABLE_SAFETY_CHECKS=true
STEPFLOW_AI_ENABLE_COST_TRACKING=true

# AI 模型管理配置
STEPFLOW_AI_AUTO_REFRESH_MODELS=true
STEPFLOW_AI_REFRESH_INTERVAL=3600
STEPFLOW_AI_ENABLE_MODEL_CACHING=true
STEPFLOW_AI_CACHE_TTL=1800
STEPFLOW_AI_MAX_MODELS_PER_PROVIDER=100
```

## 扩展点

### 自定义提供商

```rust
pub trait CustomAIProvider: Send + Sync {
    async fn generate_text(&self, request: TextGenerationRequest) -> Result<TextGenerationResponse, AIError>;
    async fn chat_completion(&self, request: ChatRequest) -> Result<ChatResponse, AIError>;
    fn get_provider_name(&self) -> &str;
    fn get_supported_models(&self) -> Vec<String>;
}
```

### 自定义执行器

```rust
pub trait CustomAIExecutor: Send + Sync {
    async fn execute_request(&self, request: AIRequest) -> Result<AIResponse, AIError>;
    fn get_supported_models(&self) -> Vec<ModelId>;
    fn get_executor_name(&self) -> &str;
}
```

这个文档提供了 `stepflow-ai` 包的核心规范，包括 AI 模型管理、多提供商支持、安全控制和流式处理功能。该包作为 AI 工具的核心组件，为 Stepflow Tool System 提供了完整的 AI 集成支持。 